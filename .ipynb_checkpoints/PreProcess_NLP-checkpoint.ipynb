{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6246006-0fbb-48d3-9036-1649563380ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz\n",
    "import spacy\n",
    "import requests\n",
    "import IPython\n",
    "import pathlib\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from keybert import KeyBERT\n",
    "#nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d06158e-8291-437a-9fcc-d9de767967dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brauch ich das noch?\n",
    "def word_count(text):\n",
    "    counts = dict()\n",
    "    words = text.split(\" \")\n",
    "\n",
    "    #print(words)\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return words, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2c408a8-2cd5-4192-921e-0c3a03629489",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "each_word_count = []\n",
    "# Einlesen der PDF's\n",
    "path = ('C:/Users/Jana/OneDrive/Dokumente/Desktop/Uni/Master/Thesis/PaperDatenextraktion/') \n",
    "p = pathlib.Path(path)\n",
    "files_path = list(p.glob('*.pdf'))\n",
    "\n",
    "#Iteration über PDF's und in text speichern. Alle PDF's liegen in text_list\n",
    "for file in files_path:\n",
    "    cur_reader = fitz.open(file)\n",
    "    text = ''\n",
    "    for page in cur_reader:  \n",
    "        text += (page.get_text(\"text\"))\n",
    "\n",
    "    text_list.append(text)\n",
    "    cur_reader.close()\n",
    "\n",
    "    \n",
    "for text in text_list: #brauch ich das noch?\n",
    "    each_word_count.append(word_count(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ed12079-27ff-4d46-ae6a-7df940e089e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Englische Stop Words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#Hinzufügen eigener Stopwords\n",
    "new_words = [\"http\", \"www\", \"among\", \"researchgate\", \"doi\", \"article\", \"author\", \"citation\", \"also\", \"see\", \"read\", \"working\", \"related\", \"introduction\", \"net\", \"tion\", \"ing\", \"use\", \n",
    "             \"ment\"]\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc9fdc80-bbfa-490a-9dd0-1e55f6e1a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "keywords = []\n",
    "#Herauszufilternde Satzzeichen\n",
    "punctuations = ['(','.',')',',','[',']',';',':','_','*','']\n",
    "#Lemma und Stemma\n",
    "lem = WordNetLemmatizer()\n",
    "ps= PorterStemmer() ##Stemming\n",
    "\n",
    "#iteration über alle PDF's\n",
    "for i in range(len(text_list)):\n",
    "    text = text_list[i] \n",
    "    text = text.lower() #alles kleinschreiben\n",
    "    tokens = word_tokenize(text) #tokens auf Wortebene\n",
    "   #Iteration über alle tokens pro PDF\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", tokens[i]) #entfernt tags\n",
    "        tokens[i] = re.sub(\"(\\\\d|\\\\W)+\",\"\", tokens[i])   #entfernt Zahlen und spezielle Satzzeichen, toDo: Bindestrich hinbekommen\n",
    "        #Entfernen einzelner und 2 Buchstaben\n",
    "        if(len(tokens[i]) <= 2):\n",
    "           tokens[i] = ''\n",
    "\n",
    "    #tokens = [lem.lemmatize(word) for word in tokens if not word in stop_words] #Lemmatisation ist das eine gute Idee?\n",
    "    tokens = [ps.stem(word) for word in tokens if not word in stop_words] #Lemmatisation ist das eine gute Idee?\n",
    "\n",
    "    keywords.append([word for word in tokens if not word in stop_words and not word in punctuations]) #Erstellen keywords (preprocessed) ohne Satzzeichen und Stop Words ect.\n",
    "    corpus.append(text) # pre Processed Corpus -> gesamter Text in kleinbuchstaben\n",
    "    \n",
    "#try keyphrasevectorizer\n",
    "#try keybert\n",
    "erg = []\n",
    "vectorizer = KeyphraseCountVectorizer()\n",
    "#Erstellt mir ngrams vom Text, sind auch keywords aus n-grammen. Das vielleicht als Attribut an die paper? bläht Graph nicht auf und danach kann dann gesucht werden? \n",
    "#ToDo: Funktion weiter untersuchen und vielleicht an Parametern basteln\n",
    "df_keyword_bert = pd.DataFrame()\n",
    "for i in range(len(corpus)): \n",
    "    kw_model = KeyBERT()\n",
    "    #document_keyphrase_matrix = vectorizer.fit_transform([corpus[i]]).toarray()\n",
    "    #keyphrases = vectorizer.get_feature_names_out()\n",
    "    #erg.append(keyphrases)\n",
    "    #print(keyphrases)\n",
    "    ergx = kw_model.extract_keywords(docs=corpus[i], vectorizer=vectorizer) #, keyphrase_ngram_range=(1,2)\n",
    "    df_ergx = pd.DataFrame(ergx)\n",
    "    pd.concat([df_keyword_bert, df_ergx])\n",
    "    #df_keyword_bert.to_csv(str(i)+'_keyword_bert_nGram.csv')\n",
    "print(df_keyword_bert) #die Ergebnisse nutzbar machen um sie als Attribute anndie Paper anzuhängen um danach suchen zu können\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fa70944-5f05-4a75-a421-bc66610dab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightage(word,text,number_of_documents=1):\n",
    "    \n",
    "    number_of_times_word_appeared = len(word_list)\n",
    "\n",
    "    tf = number_of_times_word_appeared/float(len(text))\n",
    "\n",
    "    idf = np.log((number_of_documents)/float(number_of_times_word_appeared))\n",
    "    \n",
    "    tf_idf = tf*idf\n",
    "    \n",
    "    return number_of_times_word_appeared, tf, idf, tf_idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7aaed3c4-481c-4349-a889-9e8f43c940c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#def x(corpus, keywords): #wenn ich es als Funtkion mache bekomme ich andere Ausgabe. Aber wieso ?!\n",
    "#Iteration über Texte\n",
    "for i in range(len(corpus)):\n",
    "    df_keys = {'keywords': [], #Erstellung Dictionary für nachher DataFrame\n",
    "              'number_of_times_word_appeared': [],\n",
    "              'idf': [],\n",
    "              'tf': [],\n",
    "              'tf_idf': []}\n",
    "    for keyword in keywords[i]: #Iteration über keywords pro Text\n",
    "        word_list = re.findall(keyword,corpus[i]) #Finde die keywords wieder im Text\n",
    "        if len(word_list) >= 1: #Falls keyword gefunden\n",
    "            curArray = []\n",
    "            curArray = weightage(keyword, corpus[i])    \n",
    "            \n",
    "            #if keyword not in df_keys.get('keywords') : #funktioniert so nicht\n",
    "            #Füllung des Dictionary anhand der Funktion weightage\n",
    "            df_keys['keywords'].append([keyword])\n",
    "            df_keys['number_of_times_word_appeared'].append(curArray[0])\n",
    "            df_keys['tf'].append(curArray[1])\n",
    "            df_keys['idf'].append(curArray[2])\n",
    "            df_keys['tf_idf'].append(curArray[3])\n",
    "#Umwandlung in Dataframe\n",
    "    df = pd.DataFrame(df_keys)\n",
    "    \n",
    "    df=df.loc[df.astype(str).drop_duplicates(subset='keywords').index]\n",
    "    df = df.sort_values('tf_idf',ascending=True)\n",
    "    df.head(25).to_csv(str(i)+'_Keywords.csv') #Erstellung csv pro Text\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901f111-f474-4c5d-8cd8-db0faac4a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(max_df=3, min_df=0, stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "fittedStopwords = [\"dummy\" for dummy in range(len(corpus))]\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i] = [corpus[i]]\n",
    "    fittedStopwords[i] = cv.fit_transform(corpus[i])\n",
    "print(fittedStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12e21301-1389-4f15-bebb-c9c2675c58bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026927709579467773,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 1154,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f93aff3d2e84f7fb3af2eba61bf105d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020947694778442383,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 1629486723,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85547895074f458e9fd54ad596168ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02977609634399414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 26,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb26ac8736d440d09a6c62186152de7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012967586517333984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 898822,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e680f2a90af74756a1f06d9a33330573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016347169876098633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6eeb5812dd34d508cf153b4d2c3661a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023568391799926758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 1355863,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a024b6748834e8e8235c7095ebb218b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m candidate_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Topic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# device=0 for GPU usage\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero-shot-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large-mnli\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Sample code to see labels for first five rows in df\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corpus)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:684\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    682\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_extractor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m feature_extractor\n\u001b[1;32m--> 684\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline_class(model\u001b[38;5;241m=\u001b[39mmodel, framework\u001b[38;5;241m=\u001b[39mframework, task\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:65\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__init__\u001b[1;34m(self, args_parser, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args_parser\u001b[38;5;241m=\u001b[39mZeroShotClassificationArgumentHandler(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser \u001b[38;5;241m=\u001b[39m args_parser\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentailment_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     67\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to determine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentailment\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m label id from the label2id mapping in the model config. Setting to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:770\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, model, tokenizer, feature_extractor, modelcard, framework, task, args_parser, device, binary_output, **kwargs)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;66;03m# Special handling\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 770\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;66;03m# Update config with task specific parameters\u001b[39;00m\n\u001b[0;32m    773\u001b[0m task_specific_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_specific_params\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Output labels\n",
    "candidate_labels = [\"Topic\", \"No Topic\"]\n",
    "\n",
    "# device=0 for GPU usage\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\", device=0)\n",
    "\n",
    "# Sample code to see labels for first five rows in df\n",
    "for i in range(len(corpus)):\n",
    "  input_text = corpus[i]\n",
    "  \n",
    "  # multi_label=True will return confidence score for both labels independently \n",
    "  model_dict = classifier(input_text, candidate_labels, multi_label=True)\n",
    "\n",
    "  # Zip results to dict\n",
    "  result_dict = dict(zip(model_dict.get('labels'), model_dict.get('scores')))\n",
    "  \n",
    "  # Print confidence scores\n",
    "  print(\"Input Text for the model : \", input_text)\n",
    "  print(\"Confidence Score for Topic Class : \", result_dict.get('Topic'))\n",
    "  print(\"Confidence Score for No-Topic Class : \", result_dict.get('No Topic'), end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d21cf92-2d01-4fe7-aa9d-33e8f588cb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('iot challenges', 0.6309), ('iot domain', 0.5959), ('iot', 0.5952), ('disruptive iot', 0.59), ('iot expert group', 0.561)]\n"
     ]
    }
   ],
   "source": [
    "print(keywords_attr_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31121cd-4037-48ff-969d-ff9b50d30514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
