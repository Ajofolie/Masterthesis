{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7888413-4d9c-429f-b7a0-ebd85a9f60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import numpy\n",
    "import PyPDF2\n",
    "import csv\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt') #nltk.tokenize.punkt module.  https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "nltk.download('averaged_perceptron_tagger') #nltk.tag.perceptron module, https://explosion.ai/blog/part-of-speech-pos-tagger-in-python\n",
    "nltk.download('maxent_ne_chunker') #https://www.kaggle.com/datasets/nltkdata/maxent-ne-chunker , https://www.tomaarsen.com/nltk/api/nltk.chunk.named_entity.html\n",
    "nltk.download('words') #list of str https://www.nltk.org/api/nltk.corpus.reader.html?highlight=words#nltk.corpus.reader.AlignedCorpusReader.words\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488be6d7-ae76-4252-bb30-886643792e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(pdf):\n",
    "\n",
    "    #sent_seg=sent_tokenize(pdf)\n",
    "    sent_seg = nlp(pdf)\n",
    "    for sentence in sent_seg.sents:\n",
    "        print(\"Segmentierte S채tze\", sentence)\n",
    "       \n",
    "    #print(\"Segmentierte S채tze\\n\", sent_seg_sep, \"\\n\")\n",
    "\n",
    "#tokens=[word.lower() for word in tokens if word.isalpha()]\n",
    "   # pdf_ent = nlp(pdf)\n",
    "   # print(\"named entities\\n\")\n",
    "   # for ent in pdf_ent.ents:\n",
    "      #  print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "        \n",
    "   # tokens = nltk.word_tokenize(pdf)\n",
    "   #tokens = [''.join(letter for letter in word if letter not in string.punctuation) for word in tokens if word]\n",
    "#print(tokens)\n",
    "\n",
    "# Herasufiltern Stop words\n",
    "   # filtered_sent = []\n",
    "  #  for w in tokens:\n",
    "    #    if w not in stopset:\n",
    "   #         filtered_sent.append(w)\n",
    "   # print(\"\\n S채tze ohne Stopwords\\n\", filtered_sent)\n",
    "\n",
    "    #exmpl_postagged = nltk.pos_tag(filtered_sent)\n",
    "   # print(\"\\n S채tze Postagged\\n\", exmpl_postagged)\n",
    "\n",
    "    #exmpl_chunks = nltk.ne_chunk(exmpl_postagged)\n",
    "   # print(\"\\n Chunks\\n\", exmpl_chunks)\n",
    "\n",
    "    return(sent_seg)\n",
    "\n",
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "def get_relation(sent):\n",
    "    doc = nlp(sent)\n",
    "    # Matcher class object\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    relation=[]\n",
    "    # define the pattern\n",
    "    pattern = [{'DEP': 'ROOT'},\n",
    "               {'DEP': 'prep', 'OP': \"?\"},\n",
    "               {'DEP': 'agent', 'OP': \"?\"},\n",
    "               {'POS': 'ADJ', 'OP': \"?\"}]\n",
    "\n",
    "    matcher.add(\"matching_1\", [pattern], on_match=None)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "  \n",
    "    for mathc_id, start, end in matches:\n",
    "        matched_span = doc[start: end]\n",
    "        # print(matched_span.text)\n",
    "        relation.append(matched_span.text)\n",
    "    return relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39a6be-a838-4a50-b62b-72a1a6ac9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in entity_pairs]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in entity_pairs]\n",
    "\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n",
    "kg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951d11e-8461-43fb-8635-a552619d3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_seg=preprocess(text)\n",
    "print(sent_seg)\n",
    "\n",
    "#zu csv und schreiben\n",
    "#df_csv = pd.DataFrame(data={\"sentence\": sent_seg})\n",
    "#print(df_csv)\n",
    "#df_csv.to_csv(\"C:/Users/Jana/OneDrive/Dokumente/Desktop/Uni/Master/Thesis/Test/sentence_as_csv.csv\", sep=';',index=False)\n",
    "\n",
    "#print(df_csv.shape)\n",
    "#df_csv['sentence'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134cf447-17c0-4899-a36d-48a9fd3eeec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs = []\n",
    "\n",
    "for i in tqdm(df_csv[\"sentence\"]):\n",
    "  entity_pairs.append(get_entities(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186e12cc-016b-4ce8-86a3-6b921b52b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ecb178-d8e0-409e-bf4e-72ba2be18eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [get_relation(i) for i in tqdm(df_csv['sentence'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad4ac5-66ff-497f-b224-0f701c8a6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(relations).value_counts()[:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
